#!/usr/bin/env python3.5
#
# Copyright (C) 2016,2017 The University of Sheffield, UK
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.
#

import getopt
import os
import sys
import tarfile
import time
import tempfile
from functools import partial
import fnmatch
from multiprocessing import Pool
import logging
import datetime

from ExtensionCrawler.archive import update_db_incremental
from ExtensionCrawler.config import *
from ExtensionCrawler.util import log_info, log_warning, log_error, log_exception

from ExtensionCrawler.dbbackend.mysql_backend import MysqlBackend

def help():
    print("""create-db [OPTION]""")
    print("""  -h                  print this help text""")
    print("""  -a <DIR>            archive directory""")
    print("""  -p <PREFIX>         three-letter-prefix""")
    print("""  -e <EXTIDFILELIST>  file with extension ids""")
    print("""  --from-date <DATE>  only process information gathered after"""
          """                      this date (compared lexographically)""")
    print("""  --until-date <DATE> only process information gathered before"""
          """                      this date (compared lexographically)""")
    print("""  -t <THREADS>        number of parallel threads""")
    print("""  -n <TASKID>         process chunk n where n in [1,N]""")
    print("""  -N <MAXTASKID>      """)


def process_id(from_date, until_date, path):
    start = time.time()
    with tempfile.TemporaryDirectory() as tmpdir:
        with tarfile.open(path) as t:
            t.extractall(tmpdir)

            extid = os.listdir(tmpdir)[0]
            log_info("Start processing extension", 0, extid)
            iddir = os.path.join(tmpdir, extid)

            try:
                with MysqlBackend(
                        extid, read_default_file=const_mysql_config_file(),
                        charset='utf8mb4',
                        compress=True
                ) as con:
                    for date in sorted(os.listdir(iddir)):
                        if (from_date is not None and date < from_date) or \
                                (until_date is not None and date > until_date):
                            log_info("* Skipping {}".format(date), 2, extid)
                            continue
                        try:
                            update_db_incremental(iddir, extid, date, con)
                        except Exception:
                            log_exception(
                                "Exception when handling data from {}".format(date), 0,
                                extid)
            except Exception:
                log_exception("Exception when handling extension", 0, extid)
    log_info(
        "Finished extension in {}".format(
            str(datetime.timedelta(seconds=int(time.time() - start)))),
        0,
        extid)


def find(archive, pattern):
    for root, _, files in os.walk(os.path.join(archive, "data")):
        for file in files:
            if fnmatch.fnmatch(file, pattern + ".tar"):
                yield os.path.join(root, file)


def find_from_file(archive, extidlistfile):
    with open(extidlistfile, 'r') as f:
        extids = [l.strip() for l in f.readlines()]

    for root, _, files in os.walk(os.path.join(archive, "data")):
        for file in files:
            for extid in extids:
                if fnmatch.fnmatch(file, extid + ".tar"):
                    yield os.path.join(root, file)


def parse_args(argv):
    archive = const_basedir()
    parallel = 8
    taskid = 1
    maxtaskid = 1
    from_date = None
    until_date = None

    paths = []

    try:
        opts, args = getopt.getopt(argv, "ha:p:e:t:n:N:", [
            "archive=", "prefix=", "extidlistfile=", "threads=", "taskid=",
            "maxtaskid=", "from-date=", "until-date=", "help"
        ])
    except getopt.GetoptError:
        help()
        sys.exit(2)
    for opt, arg in opts:
        if opt in ("-h", "--help"):
            help()
            sys.exit()
        elif opt in ("-a", "--archive"):
            archive = arg
        elif opt in ("-p", "--prefix"):
            paths += find(archive, arg + "*")
        elif opt in ("-e", "--extidlistfile"):
            paths += find_from_file(archive, arg)
        elif opt in ("-t", "--threads"):
            parallel = int(arg)
        elif opt in ("-n", "--taskid"):
            taskid = int(arg)
        elif opt in ("-N", "--maxtaskid"):
            maxtaskid = int(arg)
        elif opt in ("--from-date"):
            from_date = arg
        elif opt in ("--until-date"):
            until_date = arg

    if paths == []:
        paths = list(find(archive, "*"))

    chunksize = int(len(paths) / maxtaskid)
    if taskid == maxtaskid:
        paths = paths[(taskid - 1) * chunksize:]
    else:
        paths = paths[(taskid - 1) * chunksize:taskid * chunksize]

    return paths, parallel, from_date, until_date


def main(argv):
    logging.basicConfig(level=logging.INFO, format=const_log_format())

    paths, parallel, from_date, until_date = parse_args(argv)

    with Pool(processes=parallel) as p:
        p.map(partial(process_id, from_date, until_date), paths)


if __name__ == "__main__":
    main(sys.argv[1:])
