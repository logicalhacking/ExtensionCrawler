#!/usr/bin/env python3.6
#
# Copyright (C) 2018 The University of Sheffield, UK
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.
#

import sys
import getopt
import os
import sqlite3
from itertools import groupby
import time
from multiprocessing import Pool, cpu_count


class SimhashBucket:
    """Implementation of http://wwwconference.org/www2007/papers/paper215.pdf"""

    def __init__(self, nr_of_tables):
        self.tables = nr_of_tables * [{}]

        # So far, we support the variants with 4 and 20 tables. Each element of splitters
        # describes the key for one table. The first element of the tuple indicates the number
        # of bits that we shift the simhash to the right; the second element indicates how many
        # bits, from the right side, we end up taking.
        if nr_of_tables == 4:
            self.splitters = [[(0, 16)], [(16, 16)], [(32, 16)], [(48, 16)]]
        elif nr_of_tables == 20:
            block_sizes = [11, 11, 11, 11, 10, 10]
            self.splitters = []
            for i in range(0, len(block_sizes)):
                for j in range(i + 1, len(block_sizes)):
                    for k in range(j + 1, len(block_sizes)):
                        self.splitters += [[
                            (sum(block_sizes[i+1:]), block_sizes[i]),
                            (sum(block_sizes[j+1:]), block_sizes[j]),
                            (sum(block_sizes[k+1:]), block_sizes[k]),
                        ]]
        else:
            raise Exception(f"Unsupported number of tables: {nr_of_tables}")


    def bit_count(self, n):
        return bin(n).count("1")

    def get_chunk(self, n, i):
        """Reduces the simhash to a small chunk, given by self.splitters. The chunk will
        then be compared exactly in order to increase performance."""
        sum = 0
        for (s, c) in self.splitters[i]:
            sum <<= c
            sum += (n >> s) & (pow(2, c) - 1)
        return sum

    def add(self, fp):
        for i, tbl in enumerate(self.tables):
            fp_chunk = self.get_chunk(fp[0], i)
            if not fp_chunk in tbl:
                tbl[fp_chunk] = []
            tbl[fp_chunk] += [fp]

    def query(self, q):
        for i, tbl in enumerate(self.tables):
            q_chunk = self.get_chunk(q, i)
            if q_chunk in tbl:
                for fp in tbl[q_chunk]:
                    diff = self.bit_count(q ^ fp[0])
                    if diff < 4:
                        yield (fp, diff)

    def addMany(self, fps):
        for fp in fps:
            self.add(fp)

    def queryMany(self, qs):
        for q in qs:
            for (fp, diff) in self.query(q):
                yield (fp, diff)


def groupby_first(xs, n):
    return ((x, [y[n:] for y in y]) for x, y in groupby(xs, key=lambda x: x[:n]))


def get_cdnjs_simhashes(db_path, limit=None):
    with sqlite3.connect(db_path) as db:
        db.row_factory = sqlite3.Row
        for row in db.execute("select simhash, library, path, size, typ from cdnjs where "
                    "simhash IS NOT NULL AND path like '%.js' and "
                    "HEX(md5) <> 'd41d8cd98f00b204e9800998ecf8427e'" +
                            (f" LIMIT {int(limit)}" if limit is not None else "")):
            yield row


def get_crxfile_simhashes(db_path, extension_limit=None, crxfile_limit=None):
    with sqlite3.connect(db_path) as db:
        db.row_factory = sqlite3.Row
        for row in db.execute(("select extid, date, crx_etag, path, size, typ, simhash from "
                # "((extension e1 join (select extid, max(date) as date from extension group by extid {}) as e2 using (extid,date)) inner join crxfile using (crx_etag)) inner join libdet using (md5,typ) where "
                #"((select * from extension e1 where date=(select max(date) from extension e2 where e1.extid=e2.extid group by extid) order by extid {}) as d1 inner join "
                "((select extid, max(date) as date, crx_etag from extension group by extid {}) as d1 inner join "
                "(select * from crxfile where simhash is not null and path like '%.js' order by crx_etag, path) as d2 using (crx_etag)) inner join "
                "(select * from libdet where size >= 1024) as d3 using (md5,typ) {}").format(
            "LIMIT " + str(int(extension_limit)) if extension_limit is not None else "",
            "LIMIT " + str(int(crxfile_limit)) if crxfile_limit is not None else "",
                )):
            yield row

def process(tup):
    (extid, date, crx_etag), rest = tup

    result = []
    for path, rest2 in groupby_first(rest, 1):
        for size, typ, simhash in rest2:
            for ((_, (lib_path, lib_size, lib_typ)), diff) in gbl_bucket.query(simhash):
                result += [(extid, date, crx_etag, path, size, typ, lib_path, lib_size, lib_typ, diff)]
    return result


def print_help():
    print("""simhashbucket [OPTION] <DB_PATH>""")
    print("""  -h, --help                  print this help text""")
    print("""  --limit-cdnjs <N>           only retrieve N rows""")
    print("""  --limit-extension <N>       only retrieve N rows""")
    print("""  --limit-crxfile <N>         only retrieve N rows""")
    print("""  -t <THREADS>                number of parallel threads""")
    print("""  --tables <N>                number of tables to use for the bucket (4 or 20 so far)""")

def parse_args(argv):
    limit_cdnjs = None
    limit_extension = None
    limit_crxfile = None
    read_default_file = os.path.expanduser("~/.myslave.cnf")
    parallel = cpu_count()
    tables = 20

    try:
        opts, args = getopt.getopt(argv, "ht:", [
            "limit-cdnjs=", "limit-extension=", "limit-crxfile=", "read-default-file=", "help", "parallel=", "tables="])
    except getopt.GetoptError:
        print_help()
        sys.exit(2)
    for opt, arg in opts:
        if opt == "--limit-cdnjs":
            limit_cdnjs = int(arg)
        elif opt == "--limit-extension":
            limit_extension = int(arg)
        elif opt == "--limit-crxfile":
            limit_crxfile = int(arg)
        elif opt == "--read-default-file":
            read_default_file = arg
        elif opt in ("-t", "--parallel"):
            parallel = int(arg)
        elif opt == "--tables":
            tables = int(arg)

    if len(args) != 1:
        print_help()
        sys.exit(2)
    db_path = args[0]

    return limit_cdnjs, limit_extension, limit_crxfile, read_default_file, parallel, tables, db_path


def init(bucket):
    global gbl_bucket
    gbl_bucket = bucket


def main(args):
    limit_cdnjs, limit_extension, limit_crxfile, read_default_file, parallel, tables, db_path = parse_args(args)
    bucket = SimhashBucket(tables)

    start_build = time.time()
    bucket.addMany(((int(row["simhash"]), (row["path"], row["size"], row["typ"])) for row in get_cdnjs_simhashes(db_path, limit_cdnjs)))
    print(f"Building the bucket took {format(time.time() - start_build, '.2f')} seconds")

    start_query = time.time()

    data = ((row["extid"], row["date"], row["crx_etag"], row["path"], row["size"], row["typ"], int(row["simhash"])) for row in get_crxfile_simhashes(db_path, limit_extension, limit_crxfile))
    with Pool(parallel, initializer=init, initargs=(bucket,)) as p:
        for tups in p.imap_unordered(process, groupby_first(data, 3), 100):
            for tup in tups:
                sys.stdout.write("|".join(str(tup)) + "\n")
                sys.stdout.flush()
    print(f"The query took {format(time.time() - start_query, '.2f')} seconds")


if __name__ == "__main__":
    main(sys.argv[1:])
